---
title: "Real Estate Prices, Past and Future"
date: "`r Sys.Date()`"
author: "Bob Ding"
header-includes:
  - \usepackage{bm}
  - \usepackage{amsmath}
  - \usepackage{nccmath}
  - \usepackage{amssymb}
  - \usepackage{dsfont}
  - \usepackage{media9}
  - \usepackage{graphicx}
  - \usepackage{MnSymbol,wasysym}
  - \usepackage{subcaption}     
  - \usepackage{textcomp}
  - \usepackage{wrapfig}
  - \usepackage{enumitem}
  - \usepackage{fancyhdr}
  - \usepackage{float}
  - \usepackage{bm}
  - \usepackage{afterpage}
  - \usepackage{hyperref}
  - \usepackage{lscape}
  - \usepackage{tabularx}
  - \usepackage[ruled,vlined]{algorithm2e}
output: 
  pdf_document:
    fig_caption: yes
      
---

\newcommand{\bP}{\mathbb{P}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\cD}{\mathcal{D}}

\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bX}{\bm{X}}

\newcommand{\A}{\mathbf{A}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\balfa}{\bm{\alpha}}
\newcommand{\bx}{\bm{x}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bfC}{\mathbf{C}}
\newcommand{\bW}{\bm{W}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\T}{\boldsymbol{\top}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\bTheta}{\bm{\Theta}}
\newcommand{\bnu}{\bm{\nu}}
\newcommand{\balpha}{\bm{\alpha}}

```{r load packages, include=F, echo=F}
library(FedData) # Please Install this !!
pkg_test("dplyr")
pkg_test("ggplot2")
pkg_test("TSA")
pkg_test("GGally")
pkg_test("lubridate")
pkg_test("reshape2")
pkg_test("ggcorrplot")
pkg_test("geosphere")
pkg_test("tinytex")
pkg_test("fastDummies")
pkg_test("MASS")
pkg_test("gridExtra")
pkg_test("matrixStats")
pkg_test("MLmetrics")
pkg_test("kableExtra")
pkg_test("bookdown")
pkg_test("tinytex")
#tinytex::install_tinytex() turn this on first time you use it

set.seed(1234)
source("MCMCscript.R")
```


```{r load data, include=FALSE}
raw_1 <- read.csv("./Raw/output_1.csv")
raw_2 <- read.csv("./Raw/output_2.csv")
raw_3 <- read.csv("./Raw/output_3.csv")
raw_4 <- read.csv("./Raw/output_4.csv")
raw_5 <- read.csv("./Raw/output_5.csv")
raw_6 <- read.csv("./Raw/output_6.csv")
raw_7 <- read.csv("./Raw/output_7.csv")
raw_data <- do.call("rbind", list(raw_1,raw_2,raw_3,raw_4,raw_5,raw_6,raw_7))

```


\section{Introduction}

Real estate, as a major part of the economy, has been constantly and closely monitored by investors and researchers. Since 1970 \href{ https://www.dallasfed.org/-/media/documents/institute/wpapers/2014/0208.pdf }{[1]}, when most countries' statistical offices or central banks began to collect data on house prices, interest in predicting and forecasting house prices have gradually augmented, catalyzing out more and more sophisticated modeling techniques. Due to the increased ability of modern society to collect and store more data, attempts to make prediction on real estate prices have therefore shifted to data-driven, which further improved modeling precision. 

Being such a sophisticated product, real estate prices are impacted by many factors. While most of the factors helpful in predicting the house are observable and descriptive to the house itself, such as house' size, number of bathrooms, and whether it possesses a swimming pool etc., there are also not observable factors that also impact house prices, such as the underlying real state market economy, periodicity of real estate prices, and so on. 

Many previous researches have already proposed multiple ways of predicting and forecasting house prices. From the most simple regression methods as proposed in \href{https://www.kaggle.com/manisaurabh/house-prices-advanced-regression-technique }{[2]}, to those that account for repeated sells of houses \href{ https://rady.ucsd.edu/faculty/directory/valkanov/pub/docs/HandRE_GPTV.pdf }{[3]}, and to those that take temporal effects into consideration, such as \href{ https://medium.com/@feraguilari/time-series-analysis-modfinalproyect-b9fb23c28309 }{[4]} and \href{ https://www.ijcaonline.org/archives/volume152/number2/26292-2016911775}{[6]}. Though these studies are drastically different and researches can definitely incorporate more effects to propose more sophisticated models, each studies' focus are different and clear. Thus, it is important to make certain of the research question before creating model.

We proposes our goal of this study. we're interested in building a all-in-one model that takes in observable house data, but also assuming unobservable temporal effect from the real estate house market. To narrow down on the data usage, the only type of house that we'll be researching into is house in Durham, NC, due to the better familiarity of the terrain. There are 4 research questions including 1) how house specific observable, such as number of beds, number of bathrooms, etc. variables affect housing prices, 2) what periodic temporal effects is presented in the past house market, 3) extract past real estate market temporal effects, and 3) make short term forecast of housing prices.


\section{1. Exploratory Data Analysis}
\subsection{1.1 Data Discription}

The Dataset is scraped from Redfin official set \href{ https://www.redfin.com/ }{[5]}. Redfin is a real estate brokerage that was founded in 2004. Its website records historical purchase record of the past 3 years. We therefore scrapped these 3 years of data recorded for North Carolina, ranging from 2017 April to 2020 May. This dataset contains 6962 observation. Thanks to Redfin's meticulous data record, no missing value in any field was presented. Each observation is a recorded deal of house purchase. Therefore, rather than being subjective such as the seller's one-sided proposed selling price, the price is the recorded real deal price between customer and seller, which is objective enough for us to fit on. 

The data set contains many covariates. Among which, there are some hard-to-process string information, such as name of the community, and highly detailed geographical information which is beyond the scope of our interests. Therefore, to simplify our research, we introduce the following covariates of our interest.

\begin{table}[h]
\centering
\begin{tabular}{llllllll}
\hline 
\textbf{Name}         & \multicolumn{6}{l}{\textbf{Description}}                         & \textbf{Missing} \\ \hline
\texttt{Price}        & \multicolumn{6}{l}{the deal price of the house}                  & $0\%$   \\
\texttt{beds}         & \multicolumn{6}{l}{number of beds in the house}                  & $0\%$   \\
\texttt{sold.dat}     & \multicolumn{6}{l}{the date on which the deal is settled}        & $0\%$   \\
\texttt{baths}        & \multicolumn{6}{l}{the number of bathrooms the house has}        & $0\%$   \\
\texttt{square.feet}  & \multicolumn{6}{l}{usable area (ft$^2$) measured in square feet} & $0\%$   \\
\texttt{lot.size}     & \multicolumn{6}{l}{total area (ft$^2$) of the lot}               & $0\%$   \\
\texttt{house.age}    & \multicolumn{6}{l}{age (years) of the house when purchased}      & $0\%$   \\
\texttt{property.type}& \multicolumn{6}{l}{Townhouse, or Single family residential}      & $0\%$   \\
\texttt{latitude}     & \multicolumn{6}{l}{latitude of the house}                        & $0\%$   \\
\texttt{longitude}    & \multicolumn{6}{l}{longitude of the house}                       & $0\%$   \\ \hline
\end{tabular}
\end{table}

\subsection{1.2 Exploring Data}

```{r subset columns and trans Date format, include=F}
onlyDurham <- dplyr::filter(raw_data, city == "Durham")

fieldNames <- c("price", "beds", "sold.dat", "baths", "square.feet", "lot.size", "year.built", "property.type", "latitude", "longitude")
onlyDurham <- onlyDurham[fieldNames]
onlyDurham$log.price <- log(onlyDurham$price)
onlyDurham$date.date <- mdy(onlyDurham$sold.dat)
onlyDurham$house.age <- year(onlyDurham$date.date) - onlyDurham$year.built
```

The first impression is that the distribution of house types in category Condo, Townhouse, and Single Family Residential is highly uneven. This is shown in Appendix figure \ref{fig:uneven class}. Besides, A complete pairwise-plot has also been attached in the Appendix in figure \ref{fig:pair}. The rest of the following section will include 3 subsections respectively explaining the 3 major concerns about data assumption, including suggesions on how to address them. Besides, an additional subsection is also added to explain engineering of additional predictors and address of interactions.

```{r property type count bar plot, include=F, fig.width=4, fig.height=2}
house_type <- ggplot(data=onlyDurham, aes(x = property.type, fill=property.type), ) + geom_bar( stat = "count") + theme(axis.title.x = element_blank())
```


```{r Big Pair Plot, include=F, warning=FALSE, eval=T}
numericNamesMask <- unlist(lapply(onlyDurham, is.numeric))
numericNames <- names(onlyDurham)[numericNamesMask]
numericNamesIdx <- which(numericNamesMask)

pairPlot <- ggpairs(onlyDurham, columns=numericNames, 
            aes(color=property.type),
            lower  = list(continuous = wrap("points", alpha = 0.3, size=0.1) ),
            upper  = list(continuous = "blank"),
            diag = list(continuous = "densityDiag", discrete = "barDiag", na = "naDiag") ) 


for (i in 1:length(numericNamesIdx)) {

  # Address only the diagonal elements
  # Get plot out of plot-matrix
  inner <- getPlot(pairPlot, i, i);

  # Add ggplot2 settings (here we remove gridlines)
  inner <- inner + theme(panel.grid = element_blank()) +
    theme(axis.text.x = element_blank())

  # Put it back into the plot-matrix
  pairPlot <- putPlot(pairPlot, inner, i, i)

  for (j in 1:length(numericNamesIdx)){
    if((i==1 & j==1)){

      # Move the upper-left legend to the far right of the plot
      inner <- getPlot(pairPlot, i, j)
      inner <- inner + theme(legend.position=c(length(numericNamesIdx)-0.25,0.50)) 
      pairPlot <- putPlot(pairPlot, inner, i, j)
    }
    else{

      # Delete the other legends
      inner <- getPlot(pairPlot, i, j)
      inner <- inner + theme(legend.position="none")
      pairPlot <- putPlot(pairPlot, inner, i, j)
    }
  }
}


```

\subsubsection{1.2.1 Multi-collinearity}

Though increased number of beds in the house need not imply the increase of square feet, increasing number of baths in the house does imply the increase square feet more directly. Notice that in figure 1.2.1 below, a strong collinearity is shown between the number of baths and square feet of the house, achieving an correlation of 0.7843. Though they're not high enough for us to concern about identifiability issues, we should still be careful in the final model output for these highly correlated covariates. More pairwise distribution between variables can be found in pair plot shown at appendix at figure \ref{fig:pair}

```{r Collinearity, echo=F, fig.align="center", fig.height=2.5, fig.width=5}
# cor(onlyDurham$baths, onlyDurham$square.feet)
Collinearity <- ggplot(onlyDurham, aes(x=baths, y=square.feet)) + 
                      geom_point(aes(color=property.type)) + labs(caption = "figure 1.2.1")
Collinearity
```

\subsubsection{1.2.2 Heteroscedasticity}

While some strong collinearity and correlation are evident among some predictor variables, such as `beds`, the number of beds, and `square.feet`, the usable area of the house, accompanied with the increase in these predictor and response values is the increase of variance. Surely enough, bigger and more expensive houses exhibit greater variation in price. This violates the linear regression Homoscedasticity assumption, as shown in figure 1.2.2 below. To address this, we perform log-transformation on response variable and create a final regression response variable `log.price`, which is log of house deal price. Shown in the last line of the pair-plot, Heteroscedasticity problem is resolved without harming linear relationship between the response and predictors. It has also made some linear relationships more evident.

```{r Heteroskedasticity, echo=F, warning=F, fig.align = 'center',fig.height=2.5, fig.width=8}

bed_price <- ggplot(onlyDurham, aes(x=beds, y=price, color = property.type)) + 
             geom_point() +
             geom_smooth(method="nls",
                         formula = y ~ a * x+b, se = F,
                         method.args = list(start = list(a = 0.1, b = 0.1)) ) + 
             theme(legend.position="none") + 
              ggtitle("before log transformation")+ 
              labs(caption = "figure 1.2.2 (b)")


bed_log.price <- ggplot(onlyDurham, aes(x=beds, y=log.price, color = property.type)) + 
                geom_point(aes(color=property.type)) +
             geom_smooth(method="nls",
                         formula = y ~ a * x+b, se = F,
                         method.args = list(start = list(a = 0.1, b = 0.1)) ) + 
                theme(legend.position="top") + 
              ggtitle("After log transformation")+ 
              labs(caption = "figure 1.2.2 (b)")

gridExtra::grid.arrange(bed_price, bed_log.price, ncol=2)

```

\subsubsection{1.2.3 Stationarity}

When determining the model to capture temporal effect, it is imperative to verify whether the stationarity assumption has been satisfied. In our case, due to the fact it is unlikely that there are houses sold on each day, we need to construct the time series by windowing the raw data -- that is, we treat all the real estate deals that happened in a time window as deals happening at the same timestep. By specifying the width of our window, we can modulate and balance between flexibility in temporal effect and validity of regression coefficients. At this point, we choose window width to be 15 days.


```{r make windowed data, include=F, warning=F}
win_size = 15
onlyDurham <- onlyDurham[order(onlyDurham$date.date),]
begin <- min(onlyDurham$date.date)
end <- max(onlyDurham$date.date)
windowGrid <- seq(begin, end, win_size)

win_ex <- list()
win_log.price <- list()
win_mean.price <- c()
win_mid_date <- c()

onlyDurham$win.date = 0
for(i in 1:(length(windowGrid)-1) )
{
  win_start= windowGrid[i]
  win_end  = windowGrid[i+1]
  win_data = filter(onlyDurham, date.date >= win_start)
  win_data = filter(win_data, date.date <  win_end)
  
  win_ex[[i]] <- win_data
  win_log.price[[i]] <- win_data$log.price
  mid_date <- win_start + floor((win_start - win_end )/2)
  win_mid_date[i] <- mid_date
  win_mean.price[i] <- mean(win_data$log.price)
  onlyDurham$win.date <- ifelse(between(onlyDurham$date.date, win_start, win_end), 
                                format.Date(mid_date), onlyDurham$win.date)
}

```

```{r stationarity plot, echo=F, warning=F, fig.align = 'center',fig.height=2, fig.width=8}
lables = seq(begin, end, 30)

ggplot(data=onlyDurham, aes(group=win.date, y=log.price)) + theme(legend.position = "none") +
        geom_boxplot( aes(fill=win.date), outlier.shape = NA) +
        scale_y_continuous(limits = quantile(onlyDurham$log.price, c(0.005, 0.99))) +
        scale_x_date(breaks = lables) +
  labs(title = "log price at each time window", x= "time window", y = "log price",caption="1.3.2 (a)")

```

Under the 15-day window size, from figure 1.3.2 (a), we learn that there is a slight upward trend in the distribution of logarithm of house prices through time. Though this figure can only show distribution of log price marginalized out by all other exogenous variables, but it is helpful for us to identify a linearly increasing time trend. Facing the slightly non-stationarity upward trend, we propose 2 model choices (might add the third later if I have time): 1) ignore (no trend model), 2) adding `sold.dat` (linear trend model), the sold date, into the linear observation model as a predictor in order to capturing the linear trend using the linear model. We'll fit these 2 models and determine the final model by checking their performance and validation results.

Besides, it would also be helpful to identify the number of lags $p$ by 1) checking ACF/PACF plots and 2) making hypothesis. Below in figure 1.3.2 (b) shows the ACF/PACF plot mentioned. We observe that lag 1,2,3 are statistically significantly correlated. Therefore, we should at least propose have $p\geq 3$. However, as we also hold hypothesis that house prices might perform cycles of very long period , we further extend $p\geq 12$ given the fact that a window is now defined to be 15 days. Therefore, we will start by incorporating 12 auto-correlation terms in our model. Due to the excess amount of lags, we will design Bayesian ridge shrinkage prior for AR(p) coefficients that is explained in appendix section A.


```{r ggplot PACF plot func, include=F, warning=F}
ggplot.corr <- function(data, lag.max = 24, ci = 0.95, large.sample.size = TRUE, horizontal = TRUE,...) {
  
  require(ggplot2)
  require(dplyr)
  require(cowplot)
  
  if(horizontal == TRUE) {numofrow <- 1} else {numofrow <- 2}
  
  list.acf <- acf(data, lag.max = lag.max, type = "correlation", plot = FALSE)
  N <- as.numeric(list.acf$n.used)
  df1 <- data.frame(lag = list.acf$lag, acf = list.acf$acf)
  df1$lag.acf <- dplyr::lag(df1$acf, default = 0)
  df1$lag.acf[2] <- 0
  df1$lag.acf.cumsum <- cumsum((df1$lag.acf)^2)
  df1$acfstd <- sqrt(1/N * (1 + 2 * df1$lag.acf.cumsum))
  df1$acfstd[1] <- 0
  # df1 <- select(df1, lag, acf, acfstd)
  
  list.pacf <- acf(data, lag.max = lag.max, type = "partial", plot = FALSE)
  df2 <- data.frame(lag = list.pacf$lag,pacf = list.pacf$acf)
  df2$pacfstd <- sqrt(1/N)
  
  if(large.sample.size == TRUE) {
    plot.acf <- ggplot(data = df1, aes( x = lag, y = acf)) +
    geom_area(aes(x = lag, y = qnorm((1+ci)/2)*acfstd), fill = "#B9CFE7") +
    geom_area(aes(x = lag, y = -qnorm((1+ci)/2)*acfstd), fill = "#B9CFE7") +
    geom_col(fill = "#4373B6", width = 0.7) +
    scale_x_continuous(breaks = seq(0,max(df1$lag),6)) +
    scale_y_continuous(name = element_blank(), 
                       limits = c(min(df1$acf,df2$pacf),1)) +
    ggtitle("ACF") +
    theme_bw()
    
    plot.pacf <- ggplot(data = df2, aes(x = lag, y = pacf)) +
    geom_area(aes(x = lag, y = qnorm((1+ci)/2)*pacfstd), fill = "#B9CFE7") +
    geom_area(aes(x = lag, y = -qnorm((1+ci)/2)*pacfstd), fill = "#B9CFE7") +
    geom_col(fill = "#4373B6", width = 0.7) +
    scale_x_continuous(breaks = seq(0,max(df2$lag, na.rm = TRUE),6)) +
    scale_y_continuous(name = element_blank(),
                       limits = c(min(df1$acf,df2$pacf),1)) +
    ggtitle("PACF") +
    theme_bw()
  }
  else {
    plot.acf <- ggplot(data = df1, aes( x = lag, y = acf)) +
    geom_col(fill = "#4373B6", width = 0.7) +
    geom_hline(yintercept = qnorm((1+ci)/2)/sqrt(N), 
               colour = "sandybrown",
               linetype = "dashed") + 
    geom_hline(yintercept = - qnorm((1+ci)/2)/sqrt(N), 
               colour = "sandybrown",
               linetype = "dashed") + 
    scale_x_continuous(breaks = seq(0,max(df1$lag),6)) +
    scale_y_continuous(name = element_blank(), 
                       limits = c(min(df1$acf,df2$pacf),1)) +
    ggtitle("ACF") +
    theme_bw()
    
    plot.pacf <- ggplot(data = df2, aes(x = lag, y = pacf)) +
    geom_col(fill = "#4373B6", width = 0.7) +
    geom_hline(yintercept = qnorm((1+ci)/2)/sqrt(N), 
               colour = "sandybrown",
               linetype = "dashed") + 
    geom_hline(yintercept = - qnorm((1+ci)/2)/sqrt(N), 
               colour = "sandybrown",
               linetype = "dashed") + 
    scale_x_continuous(breaks = seq(0,max(df2$lag, na.rm = TRUE),6)) +
    scale_y_continuous(name = element_blank(),
                       limits = c(min(df1$acf,df2$pacf),1)) +
    ggtitle("PACF") +
    theme_bw()
  }
  cowplot::plot_grid(plot.acf, plot.pacf, nrow = numofrow)
}
```

```{r PACF store plot, include=F, fig.height=2, fig.width=6, warning=F}
temp <- ggplot.corr(data = win_mean.price, lag.max = 25, ci= 0.95, large.sample.size = FALSE, horizontal = TRUE)+ 
              labs(caption = "1.2.3 (b)")
```

```{r Plot PACF, echo=F, fig.height=2, fig.width=6,fig.align = 'center', warning=F}
temp
```


\subsubsection{1.2.4 Engineered Feature and Interaction}

We hold hypothesis that excessive number of beds in a house with comparatively low number of bathrooms can impact the house price. Therefore, we created the variable `room.Diff`, which means how much more bathrooms does the house have than beds. We found such feature creates distinct effects across Single Family Residential, Townhouse, and Condo in affecting log of price. In figure 1.2.4 (a), we can observe a different slop of the variable for 3 type of houses. Therefore, together with `room.Diff`, interactions between the the bed-bath difference and house type should also be added to our model.

Lastly, we believe incorporating a simple spatial effect covariable can increase prediction power. Therefore, we engineered the new variable `dist.duke`, indicating the distance from the house to Duke. This is approximately represented by shortest distance of 2 points on a sphere surface, calculated via length of ellipsoid whose 2 ends are determined by latitude and longitude of the house and Duke University. Then, as in figure 1.2.4 (b) shows, aligned with our conjecture, we anticipate longer distance to Duke leading to a lower housing price.


```{r Interaction and bed difference, echo=F, warning=F, fig.height=3, fig.width=8,fig.align = 'center', message=F}
onlyDurham$room.Diff <- onlyDurham$baths - onlyDurham$beds

interactionplot <- ggplot(data = onlyDurham, aes(x=room.Diff, y=log.price, color=property.type)) + 
  geom_point(alpha=0.8, size=1) +
  geom_smooth(method="nls",
              formula = y ~ a * x+b, se = F,
              method.args = list(start = list(a = 0.1, b = 0.1)) ) + 
              theme(legend.position="top",legend.text=element_text(size=8), 
                    legend.title=element_blank(), plot.title = element_text(size=12)) + ggtitle("bath/bed difference and house type interaction") + 
              labs(caption = "figure 1.2.4 (a)")
Duke <- c(36.0014, -78.9382)
onlyDurham$dist.duke <- distm(cbind(onlyDurham$latitude, onlyDurham$longitude), Duke , fun = distHaversine)
distanceplot <- ggplot(data = onlyDurham, aes(x=dist.duke, y=log.price)) + 
  geom_point(alpha=0.8, size=1) +
  geom_smooth(method = lm) + theme(legend.position="top") + ggtitle("Distance to Duke") + 
              labs(caption = "figure 1.2.4 (b)")

gridExtra::grid.arrange(interactionplot, distanceplot, ncol=2)

```


\section{2 Model Formulation}

We define our model as a regression model on top of a AR(P) model. After EDA, we determines response to be `log.price`: the logarithm of the house deal price. The predictors are `beds`, `baths`, `square.feet`, `lot.size`, `house.age`, `property.type`,  `room.Diff`, `dist.Duke`,`sold.dat`, and finally `room.Diff` interact with `property.type`. Besides, we define the intercept $\alpha_t$ as the time-varying not observable "market effect". The transition of $\alpha_t$ follows an AR(p) process. Therefore, the full model is defined as below. (\textbf{Note:} This model has incorporated `sold.dat` as a predictor. This correspond to the linear trend model mentioned in EDA 1.2.3 about stationarity. To construct the ignore trend model, simply remove `sold.dat` together with its $\beta$ coefficient)

\begin{align}
    y_t^{(i)} &= 
    \beta_{\text{beds}}\textbf{ beds}_t^{(i)} + \beta_{\text{sold date}}\textbf{ sold date}_t^{(i)} + \beta_{\text{beds}}\textbf{ baths}_t^{(i)} + \beta_{\text{square feet}}\textbf{ square feet}_t^{(i)} + \\
    &\quad \beta_{\text{lot size}}\textbf{ lot size}_t^{(i)} + \beta_{\text{house age}}\textbf{ house age}_t^{(i)}+ \beta_{\text{property type}} \textbf{ property type}_t^{(i)} +\\
    &\quad \beta_{\text{room difference}}\textbf{ room Diff}_t^{(i)}+ \beta_{\text{dist. to Duke}}\textbf{ dist. to Duke}_t^{(i)}+ \\
    &\quad \beta_{\text{interaction}}\textbf{ room Diff}_t^{(i)}\times \textbf{property type}_t^{(i)} + \alpha_t + \nu_{t}^{(i)}\\
    \alpha_t &= \sum_{i=1}^p \theta_i \alpha_{t-i} + \omega_t \\
    \omega_t &\sim \cN(0, w)\\
    \nu_{t}^{(i)} &\sim \cN(0, v)
\end{align}

Where $y_{t}^{(i)}$ is the response of the $i^{th}$ house sold on the $t^{th}$ timestep (window), which is the logarithm of deal price. (note: suppose for each window $t \in \{1,2,3,\cdots, T\}$, there are $n_t$ sold houses in the $t^{th}$ window. Then $n_t$ need not be equal for all $t$). The rests are predictive variables. $\alpha_t$ is an time varying intercept which will be modeled by the AR($p$) model described in $(5), (6)$. $\nu_{t}^{(i)}$ is an additional observation uncertainty and $\omega_t$ is an additional evolution uncertainty. To simply our modeling process, we take $\nu_t^{(i)}$, $\omega$ to have constant variance at all time.

Through reparametrization, we can simplify the model as the following. A detailed explanation of why we should formulate the model in this compact form is due to inference. Detailed explanation can be found in appendix B.

\begin{align*}
    \balfa_t &= \bTheta\balfa_{t-1} + \bm{W}_t\\
    \bm{y}_{t} &= \bm{1} \balfa_t + \bbeta \bX_t + \bnu_t\\
    \bm{W}_t &\sim \cN(\bm{0} , w\bm{I})\\
    \bnu_t &\sim \cN(\bm{0}, v\bm{I}) \\
    \bm{1} := 
    \begin{bmatrix}
    1& 0& 0& 0& 0\\
    1& 0& 0& 0& 0\\
    1& 0& 0& 0& 0\\
    \vdots& \vdots& \vdots& \vdots& \vdots\\
    1& 0& 0& 0& 0\\
    \end{bmatrix}
    &\quad 
    \bm{\alpha}_t := 
    \begin{bmatrix}
    \alpha_t\\
    \alpha_{t-1}\\
    \alpha_{t-2}\\
    \vdots\\
    \alpha_{t-p}\\
    \end{bmatrix}
    = 
    \begin{bmatrix}
    \theta_1&\theta_2   &\theta_3   &\dots  &\theta_p\\
    1       &0          &0          &\dots  &0      \\
    0       &1          &0          &\dots  &0      \\
    \vdots  & \vdots    & \vdots    &\vdots &\vdots\\
    0       &0          &0          &\dots  &0      \\
    \end{bmatrix}
    \begin{bmatrix}
    \alpha_{t-1}\\
    \alpha_{t-2}\\
    \alpha_{t-3}\\
    \vdots\\
    \alpha_{t-p}\\
    \end{bmatrix}
    =
    \bTheta \balfa_{t-1}
\end{align*}



\section{3 Methodology}

In this section, we explore methodology of using MCMC sampling with Forward Backward algorithm to make inference on parameters. Besides, we'll also have a discussion on how the methodology and the inferred parameter posterior distribution could be used to answer our pre-stated 4 research questions in introduction. 

\subsection{3.1 Parameter Inference}

The Model requires making statistical inference of the following parameters: $\{w, v, \bbeta, \btheta, \balpha_{1:T}\}$, where $T$ is the last timestep index. To obtain these, we choose to apply MCMC sampling with forward backward algorithm within the MCMC sampler. Below is the algorithm that returns an MCMC samples for these parameters. A highly detailed derivation of sampling distribution of each step can be found in appendix A. Notice that to apply MCMC, we must design a prior distribution for all the parameters. Due to the flexibility of prior design and our previous sections stated multicollinearity problem, we've decided to propose Bayesian Ridge regression priors for both the AR($p$) coefficients $\btheta$ and also the linear regression model $\bbeta$ coefficients. Details can also be found in Appendix A. From now on, denote $\cD_t := \{\bm{X}_{1:t},\bm{y}_{1:t}\}$, all the information in the observed data from initial time to time $t$.

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Sampling distribution for $\{w, v, \bbeta, \btheta, \balpha_{1:T}\}$}
\textbf{Initialize}: $\bP((\theta_1, \dots, \theta_p)^\top ), \bP(w), \bP(v)$ via pre-set prior distribution \\
\While{not converged}
 {
    \textbf{Calculate}: posterior mean and covariance for $\balpha_{t} | \cD_t$: $m_{t}, C_{t}$ $\forall t \in {1: T}$ via forward filtering algorithm in Appendix \\
    \textbf{Sample}: $\balpha_{t} | \cD_T$ from $m_{t}^*, C_{t}^*, \forall t \in {1: T}$ by backward smoothing\\
    \textbf{Sample}: $\btheta ,(\phi = w^{-1}) | \bX, \cD_T, \bbeta$ by first sampling $(\phi = w^{-1}) | \bX, \cD_T, \bbeta$ and then $\btheta | \bX, \cD_T, \bbeta, \phi$ \\
    \textbf{Sample}: $\bbeta ,(\tau = v^{-1}) | \bX, \cD_T, \btheta$ by first sampling $(\tau = v^{-1}) | \bX, \cD_T, \btheta$ and then $\bbeta | \bX, \cD_T, \btheta, \tau$
 }
\textbf{Return}: samples of $w, v, \bbeta, \btheta_{1:T}$
\caption{parameter inference algorithm}
\end{algorithm}

\subsection{3.2 Answering Research Question}

We've proposed 4 research questions at the end of the introduction section. We address each question respectively. 

To answer question 1) -- how non-temporal descriptive variables affect housing prices, as we've conducted MCMC sampling, we've obtain a distribution of parameter $\bbeta$. Interpretation on how $\bbeta$ impacts the house price is identical to the interpretation in a basic Bayesian linear regression. We'll provide point and interval estimate for all the $\beta$s.

To address question 2) -- what periodic temporal effects is presented in the past house market, we utilize eigen-decomposition of $\bTheta$ matrix to shed light on periodicity of house market price. Notice that the MCMC sampler above returns a series of $\btheta$ samples. Thus, we've obtained the posterior distribution of $\btheta$ without a point estimate. To obtian the point estimate, we can make a Bayes estimator for $\btheta$. As we aim to find an estimator $\hat{\btheta}$ that minimizes the posterior mean square error, we simply take the mean of all $\btheta$ samples to obtain this estimator $\hat{\btheta}$. Thus, a reasonable point estimate for $\btheta$ can be constructed. Through transformation, we can get point estimate of $\bTheta$ matrix. Therefore we can perform eigen-decomposition on this point estimate.

Once we extract eigenvalues together with its eigenvectors, we observe several pairs of complex eigenvalues, which generate the periodicity. For example, suppose we observe a pair of eigenvalues $a \pm bi$, this correspond to the $\alpha_t$, the real estate market, having a sine wave cycle of $Period = \frac{2\pi}{\arcsin(b / \sqrt{a^2+b^2})}$ measured in timestep, due to Euler's formula. To re-measure the perdiod in days, simply times the window size, which is 15, with $Period$. Therefore, we obtain the cycle effect. As there could be multiple cycle effects, each of their contributions are measured in magnitude of the sine wave, which can be calculated by the eigenvalue's modulus $\sqrt{a^2+b^2}$

To address question 3) -- how to extract the past real estate market $\alpha_t$. In our MCMC sampler, we have also backward sampled the $\alpha_t$ trajectory. Thus, this task can be addressed by taking mean (expectation) and quantiles (uncertainty) of all the $\alpha_t$ samples from MCMC.

To address question 4) -- how to forecast short term house prices, we're calcuating closed form distribution of $\alpha_T$ in the MCMC iteration. To forecast, linear transform this sampled closed form distribution via the sampled $\bTheta$ in that MCMC iteration to get closed form distribution of $y_{T+1}$, for example. Sample the $y_{T+1}$ from this closed form distribution, transform it back to normal price using exponent, and record it. After all iterations are over, take the sample mean of all the recorded samples (which is the predictive posterior distribution). We can obtain the predictive posterior expectation of house prices at time $T+1$. This is our prediction. Model performance (MSE) will be calculated from this mean with real data.


<!-------------------------------------------MODELING--------------------------------------------->
<!-------------------------------------------MODELING--------------------------------------------->
<!-------------------------------------------MODELING--------------------------------------------->

\section{4 Prelimiary Results}

Currently, we've generated results using 2 models 1) the ignore trend model and 2) the linear trend model. The results are not final but are reasonable. I'm attaching all the results in appendix section C.

```{r calculate cycle and period, include=F}
calPeriod <- function(eigens){
  periods <- c()
  modulus <- c()
  for(i in 1:length(eigens))
  {
    if( is.complex(eigens[i] )){
      moduli <- Mod(eigens[i])
      modulus <- append(modulus,  moduli)
      period <- 2*pi /asin( Im(eigens[i]/moduli   ) )
      periods <- append(periods, abs(period))
      } 
  }
  result <- data.frame(modulus, periods)  
  result <- na.omit(result)
  result <- unique(result)
  result <- result[is.finite(rowSums(result)),]
  return(result)
}
```

<!-------------------------------------------NO TREND--------------------------------------------->

```{r make model data no trend, include=F}

model_data_raw <- onlyDurham
model_data_raw$sold.dat <-  as.numeric(mdy(onlyDurham$sold.dat))
model_data_raw$single <- ifelse(model_data_raw$property.type == "Single Family Residential",1,0)
model_data_raw$townhouse <- ifelse(model_data_raw$property.type == "Townhouse",1,0)
model_data_raw$itr_townhouse <- model_data_raw$townhouse * model_data_raw$room.Diff
model_data_raw$itr_single <- model_data_raw$single * model_data_raw$room.Diff

fieldNames <- c("beds", "sold.dat", "baths", "square.feet", "lot.size", "house.age", "single", "townhouse", "room.Diff", "dist.duke", "itr_townhouse", "itr_single")

model_data_raw <- model_data_raw[fieldNames]

model_data_raw <- scale(model_data_raw)
center <- as.data.frame( t(attr(model_data_raw,"scaled:center")) )
scales_notrend <- as.data.frame( t(attr(model_data_raw,"scaled:scale")) )
model_data_raw <- as.data.frame(model_data_raw)
model_data_raw$log.price <-  onlyDurham$log.price


date_scaler <- scales_notrend$sold.dat

win_size <- 15
scaled_win_size <- win_size / date_scaler
begin <- min(model_data_raw$sold.dat)
end <- max(model_data_raw$sold.dat)
windowGrid <- seq(begin, end, scaled_win_size)



win_X <- list()
win_Y <- list()
win_mid_date <- c()


for(i in 1:(length(windowGrid)-1) )
{
  win_start= windowGrid[i]
  win_end  = windowGrid[i+1]
  win_data = filter(model_data_raw, sold.dat >= win_start)
  win_data = filter(win_data, sold.dat <  win_end)

  win_X[[i]] <- data.matrix(win_data[, -which(names(win_data) %in% c("sold.dat", "log.price"))])
  win_Y[[i]] <- data.matrix(win_data$log.price)
  mid_date <- win_start + floor((win_start - win_end )/2)
  win_mid_date[i] <- mid_date
}


n <- length(win_X)
pred_step <- 1

train_win_X <- win_X[c(1:(n-pred_step))]
train_win_Y <- win_Y[c(1:(n-pred_step))]
train_win_mid_date <- win_mid_date[c(1:(n-pred_step))]
test_win_X <- win_X[c((n-pred_step+1):n)]
test_win_Y <- win_Y[c((n-pred_step+1):n)]
train_win_mid_date <- win_mid_date[c((n-pred_step+1):n)]


n_train <- length(train_win_X)
n_test <- length(test_win_X)
print(n_train)
print(n_test)


```

```{r start-MCMC-notrend, include=F, results='hide'}
total_itr = 60000  #1000000
kappa = 0.1
p = 12
take <- 1000  #1000

results_notrend <- MCMC_noBlock(train_win_X, train_win_Y, test_win_X, test_win_Y, p, total_itr, kappa, take)
record_thetas <-as.data.frame(results_notrend$record_thetas)
record_betas <- as.data.frame(results_notrend$record_betas)
record_alphas <- as.data.frame(results_notrend$record_alphas)
record_ws <- as.data.frame(results_notrend$record_ws)
record_vs <- as.data.frame(results_notrend$record_vs)
record_mse <- as.data.frame(results_notrend$record_mse)
record_res <- as.data.frame(results_notrend$record_res)

thetas_mean <- as.matrix(colMeans(as.matrix(record_thetas)))

record_thetas$itr <- seq.int(nrow(record_thetas))
record_betas$itr <- seq.int(nrow(record_betas))
record_alphas$itr <- seq.int(nrow(record_alphas))
record_ws$itr <- seq.int(nrow(record_ws))
record_vs$itr <- seq.int(nrow(record_vs))

```



```{r Cyclicality-no-trend, include=F, warning=F}
jordan_mtx <- cbind(diag(p-1), rep(0, p-1))
bTHETA_mean <- rbind(t(thetas_mean), jordan_mtx)

periocity_notrend <- calPeriod(eigen(bTHETA_mean)$values)
periocity_notrend$period_in_day <- periocity_notrend$periods * win_size
rownames(periocity_notrend) <- 1:nrow(periocity_notrend)
```

```{r market traj, include=F, warning=F}
toplot_alphas <- record_alphas[, -which(names(record_alphas) == "itr")]
market_traj_notrend <- ggplot(data=melt(toplot_alphas), aes(x=variable, y=value)) + theme(legend.position = "none") +
                        geom_boxplot(outlier.shape = NA) +
                        labs(title = "market level vs time - no trend", x= "time window", y = "log price") +
                        scale_x_discrete(breaks = lables)
```



<!-------------------------------------------LINEAR TREND--------------------------------------------->

```{r make model data with sold dat, include=F}

model_data_raw <- onlyDurham
model_data_raw$sold.dat <-  as.numeric(mdy(onlyDurham$sold.dat))
model_data_raw$single <- ifelse(model_data_raw$property.type == "Single Family Residential",1,0)
model_data_raw$townhouse <- ifelse(model_data_raw$property.type == "Townhouse",1,0)
model_data_raw$itr_townhouse <- model_data_raw$townhouse * model_data_raw$room.Diff
model_data_raw$itr_single <- model_data_raw$single * model_data_raw$room.Diff

fieldNames <- c("beds", "sold.dat", "baths", "square.feet", "lot.size", "house.age", "single", "townhouse", "room.Diff", "dist.duke", "itr_townhouse", "itr_single")

model_data_raw <- model_data_raw[fieldNames]

model_data_raw <- scale(model_data_raw)
center <- as.data.frame( t(attr(model_data_raw,"scaled:center")) )
scales_linearTrend <- as.data.frame( t(attr(model_data_raw,"scaled:scale")) )
model_data_raw <- as.data.frame(model_data_raw)
model_data_raw$log.price <-  onlyDurham$log.price

date_scaler <- scales_linearTrend$sold.dat

win_size <- 15
scaled_win_size <- win_size / date_scaler
begin <- min(model_data_raw$sold.dat)
end <- max(model_data_raw$sold.dat)
windowGrid <- seq(begin, end, scaled_win_size)



win_X <- list()
win_Y <- list()
win_mid_date <- c()


for(i in 1:(length(windowGrid)-1) )
{
  win_start= windowGrid[i]
  win_end  = windowGrid[i+1]
  win_data = filter(model_data_raw, sold.dat >= win_start)
  win_data = filter(win_data, sold.dat <  win_end)

  win_X[[i]] <- data.matrix(win_data[, -which(names(win_data) %in% c("log.price"))])
  win_Y[[i]] <- data.matrix(win_data$log.price)
  mid_date <- win_start + floor((win_start - win_end )/2)
  win_mid_date[i] <- mid_date
}


n <- length(win_X)
pred_step <- 1

train_win_X <- win_X[c(1:(n-pred_step))]
train_win_Y <- win_Y[c(1:(n-pred_step))]
train_win_mid_date <- win_mid_date[c(1:(n-pred_step))]
test_win_X <- win_X[c((n-pred_step+1):n)]
test_win_Y <- win_Y[c((n-pred_step+1):n)]
train_win_mid_date <- win_mid_date[c((n-pred_step+1):n)]


n_train <- length(train_win_X)
n_test <- length(test_win_X)
print(n_train)
print(n_test)

```

```{r start-MCMC with sold dat, include=F}
kappa = 0.1

results_linearTrend <- MCMC_noBlock(train_win_X, train_win_Y, test_win_X, test_win_Y, p, total_itr, kappa, take)
record_linearTrend_thetas <-as.data.frame(results_linearTrend$record_thetas)
record_linearTrend_betas <- as.data.frame(results_linearTrend$record_betas)
record_linearTrend_alphas <- as.data.frame(results_linearTrend$record_alphas)
record_linearTrend_ws <- as.data.frame(results_linearTrend$record_ws)
record_linearTrend_vs <- as.data.frame(results_linearTrend$record_vs)
record_linearTrend_mse <- as.data.frame(results_linearTrend$record_mse)
record_linearTrend_res <- as.data.frame(results_linearTrend$record_res)

thetas_linearTrend_mean <- as.matrix(colMeans(as.matrix(record_linearTrend_thetas)))

record_linearTrend_thetas$itr <- seq.int(nrow(record_linearTrend_thetas))
record_linearTrend_betas$itr <- seq.int(nrow(record_linearTrend_betas))
record_linearTrend_alphas$itr <- seq.int(nrow(record_linearTrend_alphas))
record_linearTrend_ws$itr <- seq.int(nrow(record_linearTrend_ws))
record_linearTrend_vs$itr <- seq.int(nrow(record_linearTrend_vs))

```


```{r Cyclicality-no-with sold dat, include=F, warning=F}
jordan_mtx <- cbind(diag(p-1), rep(0, p-1))
bTHETA_mean <- rbind(t(thetas_linearTrend_mean), jordan_mtx)

periocity_linearTrend <- calPeriod(eigen(bTHETA_mean)$values)
periocity_linearTrend$period_in_day <- periocity_linearTrend$periods * win_size

rownames(periocity_linearTrend) <- 1:nrow(periocity_linearTrend)
```

```{r market traj with sold dat, include=F, warning=F}
toplot_alphas <- record_linearTrend_alphas[, -which(names(record_linearTrend_alphas) == "itr")] + unique(model_data_raw$sold.dat) * mean(record_linearTrend_betas$sold.dat)
market_traj_linearTrend <- ggplot(data=melt(toplot_alphas), aes(x=variable, y=value)) + theme(legend.position = "none") +
                        geom_boxplot(outlier.shape = NA) +
                        labs(title = "market level vs time - linear trend", x= "time window", y = "log price")+
                        scale_x_discrete(breaks = lables)
```

<!-------------------------------------------LOCALLY LINEAR--------------------------------------------->





\section{5 Model Validation and Sensitivity Analysis}

Model Validation will be 2 separate parts: 1) validation on linear regression model, and 2) MCMC validation. We've finished all the model validation and relevant figures will be attached in Appendix D.1 and D.2. Formal write up will be left to the next submission. 

Sensitivity Analysis will be focused on validating a series of Bayesian ridge regression shrinkage coefficient $\kappa$. Besides, variation of different time window lengths will also be tested.  


<!-------------------------------------------REFERENCE--------------------------------------------->
<!-------------------------------------------REFERENCE--------------------------------------------->
<!-------------------------------------------REFERENCE--------------------------------------------->


\section*{Bibliography}

\href{ https://www.dallasfed.org/-/media/documents/institute/wpapers/2014/0208.pdf }{[1]} 
Schularick, M., &amp; Steger, T. (2014). No Price Like Home: Global House Prices, 1870–2012. Federal Reserve Bank of Dallas, Globalization and Monetary Policy Institute Working Papers, 2014(208). doi:10.24149/gwp208

\href{ https://www.kaggle.com/manisaurabh/house-prices-advanced-regression-technique }{[2]}, 
Manisaurabh. (2020, September 14). House-Prices-Advanced-Regression-Technique. Retrieved October 18, 2020, from https://www.kaggle.com/manisaurabh/house-prices-advanced-regression-technique

\href{ https://rady.ucsd.edu/faculty/directory/valkanov/pub/docs/HandRE_GPTV.pdf }{[3]}, 
Ghysels, E., Plazzi, A., Valkanov, R., &amp; Torous, W. (2013). Forecasting Real Estate Prices. Handbook of Economic Forecasting, 509-580. doi:10.1016/b978-0-444-53683-9.00009-8

\href{ https://medium.com/@feraguilari/time-series-analysis-modfinalproyect-b9fb23c28309 }{[4]}.
Aguilar, F. (2019, July 15). Time Series Analysis on US Housing Data. Retrieved October 18, 2020, from https://medium.com/@feraguilari/time-series-analysis-modfinalproyect-b9fb23c28309

\href{ https://www.redfin.com/ }{[5]}.
Redfin. (2020). Real Estate, Homes for Sale, MLS Listings, Agents | Redfin. https://www.redfin.com/

\href{ https://www.ijcaonline.org/archives/volume152/number2/26292-2016911775}{[6]}. Bhagat, N., Mohokar, A., &amp; Mane, S. (2016). House Price Forecasting using Data Mining. International Journal of Computer Applications, 152(2), 23-26. doi:10.5120/ijca2016911775
\newpage

<!-------------------------------------------APPENDIX--------------------------------------------->
<!-------------------------------------------APPENDIX--------------------------------------------->
<!-------------------------------------------APPENDIX--------------------------------------------->

\appendix

\section{Appendix}

\subsection{A Parameter Inference}

\subsubsection{A.1 Forward Filtering}

\begin{align*}
    \balfa_t &= \bTheta\balfa_{t-1} + \bm{W}_t\\
    \bm{y}_{t} &= \bm{1} \balfa_t + \bbeta \bX_t + \bnu_t\\
    \bm{W}_t &\sim \cN(\bm{0} , w\bm{I})\\
    \bnu_t &\sim \cN(\bm{0}, v\bm{I}) \\
    \bm{1} &:= 
    \begin{bmatrix}
    1& 0& 0& 0& 0\\
    1& 0& 0& 0& 0\\
    1& 0& 0& 0& 0\\
    \vdots& \vdots& \vdots& \vdots& \vdots\\
    1& 0& 0& 0& 0\\
    \end{bmatrix}
\end{align*}

First, denote that 


\begin{align*}
    \balpha_t | \cD_{t-1}, - &\sim \cN(\bTheta m_{t-1}, \bTheta C_{t-1}\bTheta^T + w\bm{I}) = \cN(a_t, R_t)\\
    \bm{y}_t | \balpha_t, \cD_{t-1}, - &\sim \cN(\bm{1}\balpha_t + \bX_t \bbeta, v\bm{I})\\
    \bP(\balpha_t | \cD_t) &\propto \bP(\balpha_t | \cD_{t-1}, -)\bP(\bm{y}_t | \balpha_t, \cD_{t-1}, -)\\
    &\propto \exp\left\{-\frac{1}{2}\left[ \balpha_t^T(R_t^{-1} + v^{-1}\bm{1}^T\bm{1})\balpha - 2\balpha_t^T(R_t^{-1}a_t + v^{-1}\bm{1}^T(\bm{y}_t - \bX_t\bbeta)) \right] \right\}\\
    &\sim \cN\left(\left( R_t^{-1}+v^{-1}\bm{1}^T\bm{1}\right)^{-1}(R_t^{-1}a_t + v^{-1}\bm{1}^T(\bm{y}_t - \bX_t\bbeta)) ,\left( R_t^{-1}+v^{-1}\bm{1}^T\bm{1}\right)^{-1}\right)\\
    &= \cN(m_t, C_t)\\\\
    a_t &= \bTheta m_{t-1}\\
    R_t &= \bTheta C_{t-1}\bTheta^T + w\bm{I}\\
    m_t &= \left( R_t^{-1}+v^{-1}\bm{1}^T\bm{1}\right)^{-1}(R_t^{-1}a_t + v^{-1}\bm{1}^T(\bm{y}_t - \bX_t\bbeta))\\
    C_t &= \left( R_t^{-1}+v^{-1}\bm{1}^T\bm{1}\right)^{-1}
\end{align*}

Use this equation to update

\subsubsection{A.2 Backward Smoothing}
\tiny
Sh*t, I hate this...
\bigbreak
\normalsize
Suppose we already know that 

$$\bP(\balpha_{t+1} | \cD_T ) \sim \cN(m_{t+1}^*, R_{t+1}^*) $$


Let's look at log likelihood of $\balpha_t, \balpha_{t+1} | \cD_T$. Using conditional independence, we have


\begin{align*}
    -\frac{1}{2}\ell(\balpha_t, \balpha_{t+1} ; \cD_T) &= \log \bP\left(\balpha_{t+1} \mid \balpha_{t}\right)+\log \bP\left(\balpha_{t} \mid \cD_t\right)-\log \bP\left(\balpha_{t+1} \mid \cD_t \right)+\log \bP\left(\balpha_{t+1} \mid \cD_T\right)\\
    &= (w)^{-1}(\balpha_{t+1} - \bTheta\balpha_{t})^T(\balpha_{t+1} - \bTheta\balpha_{t}) + (\balpha_t - m_t)^T(C_t)^{-1}(\balpha_t - m_t) -\\
    &\quad (\balpha_{t+1} - a_{t+1})^T(R_{t+1})^{-1}(\balpha_{t+1} - a_{t+1}) + \\
    &\quad (\balpha_{t+1} -m_{t+1}^*)^T(C_{t+1}^*)^{-1}(\balpha_{t+1} -m_{t+1}^*) + \text{constant}\\
    &= \balpha_{t+1}^T({C_{t+1}^*}^{-1} + w^{-1}\bm{I} + R_{t+1}^{-1}) \balpha_{t+1} + \balpha_{t}^T(w^{-1}\bTheta^T\bTheta+C_t^{-1}) \balpha_t +\\
    &\quad 2\balpha_{t+1}^T(-w^{-1}\bTheta)\balpha_{t} - 2\balpha_t^T(C_t^{-1}m_t) - 2\balpha_{t+1}^T(R_{t+1}^{-1}a_{t+1} + {C_{t+1}^*}^{-1}m_{t+1}^*)+  \text{constant}
\end{align*}

One eternity of calculation later, we end up with:
\begin{align*}
    J_t &= C_t\bTheta^T(\bTheta C_t \bTheta^T + w\bm{I})^{-1}\\
    m_{t}^* &= m_{t} + J_t(m_{t+1}^* - \bTheta m_t)\\
    C_{t}^* &= C_t + J_t(C_{t+1}^* - \bTheta C_t \bTheta^T - w\bm{I})J_t^T
\end{align*}

And therefore
$$
\balpha_{t} | \balpha_{t+1}, \cD_T \sim \cN\Big(m_{t} + J_{t}(      \balpha_{t+1}  - \bTheta m_{t} ), C_{t} - J_{t}R_{t+1}J_{t}^\top\Big)
$$

\subsubsection{A.3 Dynamic model sampling: $(\theta_1, \cdots, \theta_p)^\top$, $w = \phi^{-1}$}

This is a simple linear regression $\bm{\alpha} = \mathbf{X}\btheta + w_t$ with design matrices as

\begin{align*}
    \bm{y} =
    \begin{bmatrix}
    \alpha_1\\
    \alpha_2\\
    \alpha_3\\
    \alpha_4\\
    \alpha_5\\
    \vdots\\
    \alpha_T
    \end{bmatrix} \quad \quad
    \mathbf{X} &= 
    \begin{bmatrix}
    \alpha_{1-1}& \hdots & \alpha_{1-p}\\
    \alpha_{2-1}& \hdots & \alpha_{2-p}\\
    \alpha_{3-1}& \hdots & \alpha_{3-p}\\
    \alpha_{4-1}& \hdots & \alpha_{4-p}\\
    \alpha_{5-1}& \hdots & \alpha_{5-p}\\
    \vdots      & \vdots & \vdots \\
    \alpha_{T-1}& \hdots & \alpha_{T-p}\\
    \end{bmatrix} \quad \quad 
    \btheta = 
    \begin{bmatrix}
    \theta_1\\
    \theta_2\\
    \theta_3\\
    \theta_4\\
    \theta_5\\
    \theta_6\\
    \theta_p\\
    \end{bmatrix}
\end{align*}

\begin{align*}
    \mathcal{L}(\bm{y} ; \btheta, \mathbf{X}) &\propto \phi^{\frac{T}{2}} \exp\{-\frac{1}{2}\phi ( \bm{y} - \mathbf{X}\btheta)^{\T} ( \bm{y} - \mathbf{X}\btheta)\} \\
    \btheta | \phi, \cD_T, \bbeta, v &\sim \mathcal{N}\left(\boldsymbol{\mu}_{0}, \boldsymbol{\Lambda}_{0}^{-1} / \phi\right) = \cN((0.5, 0.5, 0.5)^T, 1/3 \phi^{-1}\mathbf{I})\\
    \phi | \cD_T, \bbeta, v &\sim \mathbf{G}\left(a_0 = \frac{v_{0}}{2}, b_0 = \frac{v_0 s_0^2}{2}  \right) = \mathbf{G}\left(\frac{1}{2}, \frac{1}{2} \right)\\
    \boldsymbol{\mu}_{n} &= \left(\mathbf{X}^{\mathrm{T}}\mathbf{X}+\boldsymbol{\Lambda}_{0}\right)^{-1}\left(\boldsymbol{\Lambda}_{0} \boldsymbol{\mu}_{0}+\mathbf{X}^{\mathrm{T}} \boldsymbol{\bm{y}}\right) \\
    \boldsymbol{\Lambda}_{n}  &= \left(\mathbf{X}^{\mathrm{T}}\mathbf{X}+\mathbf{\Lambda}_{0}\right)   \\
    a_n &= a_0 + \frac{T}{2}\\
    b_n &= b_0 + \frac{1}{2}(\bm{y}^{\T}\bm{y} + \boldsymbol{\mu}_0^{\T}\boldsymbol{\Lambda}_0\boldsymbol{\mu}_0 - \boldsymbol{\mu}_n^{\T}\boldsymbol{\Lambda}_n\boldsymbol{\mu}_n) \\
    \btheta | \phi, \bX, \cD_T, \bbeta, v &\sim \mathcal{N}\left(\boldsymbol{\mu}_{n}, \boldsymbol{\Lambda}_{n}^{-1} / \phi\right)\\
    \phi | \bX, \cD_T, \bbeta, v &\sim \mathbf{G}\left(a_n, b_n \right)\\
\end{align*}

\subsubsection{A.4 Observation Model Sampling $(\beta_1 \cdots )^\top$, $v =\tau^{-1}$}

Very similar as above, this is also a linear model. Besides, it is possible to apply Bayesian Ridge here. let's create the Bayesian ridge model


\begin{align*}
    \bm{y}_t &= \bm{\alpha}_t\bm{1} + \bm{X} \bbeta + \nu_t\\
    \bm{z}_t = (\bm{y}_t-\bm{\alpha}_t\bm{1}) \mid \bm{\alpha}_t, \boldsymbol{\beta}, \tau &\sim \mathrm{N}\left(\mathbf{X} \boldsymbol{\beta}, \mathbf{l}_{n} / \tau\right) \\
    \boldsymbol{\beta} \mid \tau, \kappa & \sim \mathrm{N}\left(\mathbf{0}, \mathbf{l}(\tau \kappa)^{-1}\right) \\
    p(\tau \mid \kappa) & \propto 1 / \tau \\\\
    \bP(\bm{y}_t-\bm{\alpha}_t\bm{1} | \bX, \bbeta, \tau, \bm{\alpha}) &\propto \tau^{\frac{n}{2}}\exp\{-\frac{\tau}{2}(\bm{z}_t - \bX\bbeta)^T(\bm{z}_t - \bX\bbeta)\}\\
    \bP( \bbeta | \bX, \bm{z}_t, \tau, \bm{\alpha}) &\propto \bP(\bm{z}_t | \bX, \bbeta, \tau, \bm{\alpha})\bP(\bbeta | \tau, \kappa, \bm{\alpha})\bP(\tau | \kappa, \bm{\alpha})\\
    &\propto \tau^{\frac{n}{2}}\exp\{-\frac{\tau}{2}(\bm{z}_t - \bX\bbeta)^T(\bm{z}_t - \bX\bbeta)\} (\tau \kappa)^{\frac{p}{2}} \exp\left\{ -\frac{\tau \kappa}{2}\bbeta^T\bbeta\right\}\tau^{-1}\\
    &\propto \exp \left\{ -\frac{1}{2}\left[ \bbeta^T(\tau \bX^T\bX+ \tau\kappa\bm{1} )\bbeta - 2\tau\bbeta^T\bX^T\bm{z}_t  \right]\right\}\\
    &\sim N\left( (\bX^T\bX + \kappa \bm{1}_p)^{-1} \bX^T\bm{z}_t, \tau^{-1}(\bX^T\bX + \kappa \bm{1}_p)^{-1}\right)\\
    \bbeta | \bX, \bm{z}_t, \tau , \bm{\alpha}&= \cN(\bm{\mu}, \bm{\Sigma})\\
    \bP(\tau | \bbeta, \kappa, \bm{z}_t, \bm{\alpha}) &\propto \tau^{\frac{n+p}{2}-1}\exp\left\{ -\frac{\tau \kappa}{2}\bbeta^T\bbeta\right\}\\
    \tau | \bbeta, \kappa, \bm{z}_t, \bm{\alpha} &\sim \bm{G}(\frac{n+p}{2}, \frac{\kappa}{2}\bbeta^T\bbeta)
\end{align*}



\subsection{B Methodology}

We define our model as a regression model on top of a AR(P) model. 

\subsubsection{B.1 Regression (Observation) Model}

After EDA, we determines to use response as `log.price`: the logarithm of the house deal price. The covariate predictors are `beds`, `sold.dat`, `baths`, `square.feet`, `lot.size`, `house.age`, `property.type`,  `room.Diff`, `dist.Duke`, and finally `room.Diff` interact with `property.type`. The regression model is 

\begin{align}
    y_t^{(i)} &= 
    \beta_{\text{beds}}\textbf{ beds}_t^{(i)} + \beta_{\text{sold date}}\textbf{ sold date}_t^{(i)} + \beta_{\text{beds}}\textbf{ baths}_t^{(i)} + \beta_{\text{square feet}}\textbf{ square feet}_t^{(i)} + \\
    &\quad \beta_{\text{lot size}}\textbf{ lot size}_t^{(i)} + \beta_{\text{house age}}\textbf{ house age}_t^{(i)}+ \beta_{\text{property type}} \textbf{ property type}_t^{(i)} +\\
    &\quad \beta_{\text{room difference}}\textbf{ room Diff}_t^{(i)}+ \beta_{\text{dist. to Duke}}\textbf{ dist. to Duke}_t^{(i)}+ \\
    &\quad \beta_{\text{interaction}}\textbf{ room Diff}_t^{(i)}\times \textbf{property type}_t^{(i)} + \\
    &\quad \alpha_t + \nu_t\\
    \nu_t &\sim \cN(0, v)
\end{align}

Where $y_{t}^{(i)}$ is the response of the $i^{th}$ house sold on the $t^{th}$ window date, which is its logarithm of deal price. (notice that suppose for each window $t \in \{1,2,3, T\}$, there are $n_t$ sold houses in the $t^{th}$ window. Then $n_t$ need not equal for all $t$). The rests are predictive variables. $\alpha_t$ is an time varying intercept which will be modeled by the AR(P) model described in the next session. $\nu_t$ is an additional observation uncertainty. To simply our modeling process, we take $\nu_t$ to have constant variance. Also, to simplify our notation, we write vectorized equation by merging line $(1),(2),(3),(4)$. The compact form is denoted as

\begin{align*}
    \bm{y}_t &= \alpha_t\bm{1}_{n_t} + \bm{X}_t\bbeta + \bm{\nu}_t\\
    \bm{\nu}_t &\sim \cN(\bm{0}, v \bm{I}_{n_t})
\end{align*}

\subsubsection{B.2 Time Series Model}

We construct the AR(p) model to model the underlying intercept $\alpha_t$ as described above in the regression model. As we've already indicated in EDA section, we'll choose $p =7$ for our

\begin{align*}
    \alpha_t &= \sum_{i=1}^p \theta_i \alpha_{t-i} + \omega_t \\
    \omega_t &\sim \cN(0, w)
\end{align*}

However, the above parametrization requires us to take-in many timesteps value to predict $\alpha_t$, we may simply it by vectorizing the expression into the following:

\begin{align*}
    \bm{\alpha}_t = 
    \begin{bmatrix}
    \alpha_t\\
    \alpha_{t-1}\\
    \alpha_{t-2}\\
    \vdots\\
    \alpha_{t-p}\\
    \end{bmatrix}
    &= 
    \begin{bmatrix}
    \theta_1&\theta_2   &\theta_3   &\dots  &\theta_p\\
    1       &0          &0          &\dots  &0      \\
    0       &1          &0          &\dots  &0      \\
    \vdots  & \vdots    & \vdots    &\vdots &\vdots\\
    0       &0          &0          &\dots  &0      \\
    \end{bmatrix}
    \begin{bmatrix}
    \alpha_{t-1}\\
    \alpha_{t-2}\\
    \alpha_{t-3}\\
    \vdots\\
    \alpha_{t-p}\\
    \end{bmatrix}
    =
    \bTheta \balfa_{t-1}
\end{align*}

In this way, transition becomes easy, as dependency rely on only the past one timestep. 

\subsubsection{B.3 Combined Model}

We end up with the model as the following

\begin{align*}
    \balfa_t &= \bTheta\balfa_{t-1} + \bm{W}_t\\
    \bm{y}_{t} &= \bm{1} \balfa_t + \bbeta \bX_t + \bnu_t\\
    \bm{W}_t &\sim \cN(\bm{0} , w\bm{I})\\
    \bnu_t &\sim \cN(\bm{0}, v\bm{I}) \\
    \bm{1} &:= 
    \begin{bmatrix}
    1& 0& 0& 0& 0\\
    1& 0& 0& 0& 0\\
    1& 0& 0& 0& 0\\
    \vdots& \vdots& \vdots& \vdots& \vdots\\
    1& 0& 0& 0& 0\\
    \end{bmatrix}
\end{align*}



<!-------------------------------------------PRESENT RESULT--------------------------------------------->
\subsection{C Current Results}

\subsubsection{C.1 Answer to question 1}

```{r beta-estimates, echo=F}
# no trend
record_betas <- record_betas[, which(names(record_betas) != c("itr"))]
estimate_betas <- colMeans(record_betas) / as.matrix(scales_notrend[,-2])
estimate_beta_ci <- colQuantiles(as.matrix(record_betas), probs = c(0.025, 0.975) )
estimate_beta_ci[,1] <- estimate_beta_ci[,1] / as.matrix(scales_notrend[,-2])
estimate_beta_ci[,2] <- estimate_beta_ci[,2] / as.matrix(scales_notrend[,-2])
notrend_beta <- data.frame(cbind(t(estimate_betas), estimate_beta_ci))
names(notrend_beta) <- c("mean","0.025%", "0.975%")

emptyrow <- data.frame(NaN, NaN, NaN)
rownames(emptyrow) <- "sold.dat"
names(emptyrow) <- c("mean","0.025%", "0.975%")
notrend_beta <- rbind(notrend_beta, emptyrow)


# linear trend
record_linearTrend_betas <- record_linearTrend_betas[, which(names(record_linearTrend_betas) != c("itr"))]
estimate_LinearTrend_betas <- colMeans(record_linearTrend_betas) / as.matrix(scales_linearTrend)
estimate_beta_LinearTrend_ci <- colQuantiles(as.matrix(record_linearTrend_betas), probs = c(0.025, 0.975) ) 
estimate_beta_LinearTrend_ci[,1] <- estimate_beta_LinearTrend_ci[,1] / as.matrix(scales_linearTrend) 
estimate_beta_LinearTrend_ci[,2] <- estimate_beta_LinearTrend_ci[,2] / as.matrix(scales_linearTrend)
LinearTrend_beta <- data.frame(cbind(t(estimate_LinearTrend_betas), estimate_beta_LinearTrend_ci))
names(LinearTrend_beta) <- c("mean","0.025%", "0.975%")

beta <- cbind(notrend_beta, LinearTrend_beta)

kbl(beta, booktabs=T, caption = "Estimate for Beta") %>%
  add_header_above(c(" " = 1, "No Trend Model" = 3, "Linear Trend Model" = 3)) %>% 
  kable_styling(latex_options = "striped") %>%
  kable_styling(latex_options = "HOLD_position")

```

\subsubsection{C.2 Answer to question 2}

```{r, echo=F}
if(nrow(periocity_linearTrend) == nrow(periocity_notrend))
{
  table.notrend.periodicity <- kable(periocity_notrend)
  table.linearTrend.periodicity <- kable(periocity_linearTrend)
  periodicity <- as.data.frame(cbind(periocity_notrend, periocity_linearTrend))
  rownames(periodicity) <- seq(nrow(periodicity))
  
  
  kbl(periodicity, booktabs=T, caption = "Periodicity with Moduli (Magnitude)") %>%
    add_header_above(c("No Trend Model" = 3, "Linear Trend Model" = 3)) %>% 
    kable_styling(latex_options = "striped") %>%
    kable_styling(latex_options = "HOLD_position")  
} else
{
  kbl(periocity_notrend, booktabs=T, caption = "No Trend Model Periodicity with Moduli (Magnitude)") %>%
    kable_styling(latex_options = "striped") %>%
    kable_styling(latex_options = "HOLD_position")  
  kbl(periocity_linearTrend, booktabs=T, caption = "Linear Trend Model Periodicity with Moduli (Magnitude)") %>%
    kable_styling(latex_options = "striped") %>%
    kable_styling(latex_options = "HOLD_position")  
}
```

\subsubsection{C.3 Answer to question 3}

```{r, echo=F, fig.align=T, fig.width=8, fig.height=4}
gridExtra::grid.arrange(market_traj_notrend, market_traj_linearTrend, ncol=1)
```

\subsubsection{C.4 Answer to question 4}

```{r, echo=F}
mse_record <- data.frame( record_mse, record_linearTrend_mse)
names(mse_record) <- c("no trend", "linear trend")
kbl(mse_record, caption = "1 step prediction MSE for 2 models") %>% 
  kable_styling(latex_options = "striped") %>%
  kable_styling(latex_options = "HOLD_position")

```

\subsection{D Model Validation Figures}

\subsubsection{D.1 No Trend Model}

```{r MCMC notrend Diagnostic, echo=F}
record_betas$itr <- seq(nrow(record_betas))
MCMCtraj_beta_notrend <- melt(record_betas ,  id.vars = 'itr', variable.name = 'series')
MCMCtraj_theta_notrend <- melt(record_thetas ,  id.vars = 'itr', variable.name = 'series')
traj_v_notrend <- ggplot(data=record_vs, aes(x=itr, y=V1)) + geom_line() + ggtitle("MCMC traj v")
traj_w_notrend <- ggplot(data=record_ws, aes(x=itr, y=V1)) + geom_line() + ggtitle("MCMC traj w") 
traj_betas_notrend <- ggplot(data=MCMCtraj_beta_notrend, aes(x=itr, y=value)) + geom_line(aes(colour = series)) + ggtitle("MCMC traj betas") + theme(legend.position = "none")
traj_thetas_notrend <- ggplot(data=MCMCtraj_theta_notrend, aes(x=itr, y=value)) + geom_line(aes(colour = series)) + ggtitle("MCMC traj thetas") + theme(legend.position = "none")
grid.arrange(traj_v_notrend, traj_w_notrend, traj_betas_notrend, traj_thetas_notrend, nrow = 2)

boxplot(as.vector(record_res))
```


\subsubsection{D.2 Linear Trend Model}

```{r MCMC notrend Diagnostic with sold dat, echo=F}
record_linearTrend_betas$itr <- seq(nrow(record_linearTrend_betas))

MCMCtraj_beta_linearTrend <- melt(record_linearTrend_betas ,  id.vars = 'itr', variable.name = 'series')
MCMCtraj_theta_linearTrend <- melt(record_linearTrend_thetas ,  id.vars = 'itr', variable.name = 'series')
traj_v_linearTrend <- ggplot(data=record_linearTrend_vs, aes(x=itr, y=V1)) + geom_line() + ggtitle("MCMC traj v")
traj_w_linearTrend <- ggplot(data=record_linearTrend_ws, aes(x=itr, y=V1)) + geom_line() + ggtitle("MCMC traj w") 
traj_betas_linearTrend <- ggplot(data=MCMCtraj_beta_linearTrend, aes(x=itr, y=value)) + geom_line(aes(colour = series)) + ggtitle("MCMC traj betas")  + theme(legend.position = "none")
traj_thetas_linearTrend <- ggplot(data=MCMCtraj_theta_linearTrend, aes(x=itr, y=value)) + geom_line(aes(colour = series)) + ggtitle("MCMC traj thetas") + theme(legend.position = "none")
grid.arrange(traj_v_linearTrend, traj_w_linearTrend, traj_betas_linearTrend, traj_thetas_linearTrend, nrow = 2)

boxplot(as.vector(record_linearTrend_res))
```


\subsection{E Random Figures}

```{r, echo=F, fig.align='center', fig.cap='\\label{fig:uneven class} uneven distribution of classes'}
house_type
```
```{r, echo=F, message=F, fig.align='center', fig.cap='\\label{fig:pair} pair plot', echo=F}
pairPlot
```



{\sc Last Revised: \today}

